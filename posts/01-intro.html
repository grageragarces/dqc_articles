<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Why am I working on Distributed Quantum Computing (DQC)?</title>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <article>
    <h1>Why am I working on Distributed Quantum Computing (DQC)?</h1>
    <p><em>1st of May 2025</em></p>

    <p>
      Distributed Quantum Computing (DQC) offers a promising path to scale quantum capabilities beyond the limitations of single devices.
      In this post, I share the motivation behind my work in this area:
      why distributing quantum computations matters, what challenges it poses,
      and how it connects to real limitations in current hardware.
      This sets the stage for a broader discussion on how we can externalize,
      coordinate, and benchmark quantum computation across multiple devices.
    </p>

    <p><a href="../index.html">← Back to all posts</a></p>
  </article>
</body>

<p>
  Building ever-larger quantum computers by simply adding more qubits into a single device isn't a sustainable strategy. 
  Let's look at why: 
</p>
<p>
  No matter how small we make qubits, we're up against fundamental physical constraints.
  For instance the "area law" in quantum physics, 
  suggests that the entanglement entropy (a measure of quantum correlations) of a subsystem scales with the boundary area of the subsystem. 
  This concept was first introduced in the context of black hole thermodynamics by physicists Jacob Bekenstein and Stephen Hawking in the 1970s, who found that the entropy of a black hole is proportional to the area of its event horizon. 
  Implying that as we try to pack more qubits into a single device, the complexity of managing entanglement will grow rapidly (with the surface area of subsystems), making it increasingly difficult to maintain coherent quantum states.
  
  Decoherence is already a huge problem in current quantum computers, where achieving fault tolerance requires significant overhead - the entire field of QEC (Quantum Error Correction), was built to fight these errors. 
  QEC tries to manipulate quantum objects, which are inherently fragile and sensitive to their environment.
  Implementing error correction protocols necessitates additional physical qubits to encode a single logical qubit, increasing the total qubit counts. 
  This exacerbates the spatial constraints and introduces additional complexity in qubit control and coherence maintenance.
  We expect to reach a point where adding more qubits to a single device will yield diminishing returns, if not outright failure, due to the inability to manage the entanglement and coherence of these incredibly complex systems.
  Building wide forever is not viable. 
  Instead, we must build tall, that is, distribute computations across multiple devices rather than growing a single one indefinitely. 
  This shift allows us to scale without hitting the physical limits imposed by coherence, control, and space.
</p>

<p>
  These challenges are reminiscent of the obstacles faced in classical computing during the late 20th century. 
  In the late 20th century, classical computers hit expected physical limits - similar to the ones we are predicting in the quantum world.
  Moore's law kept pushing the number of transistors up, but at some point, packing more into a chip caused too much heat and reliability issues.
  Processors started to run into both physical limits (how small you can make features) and thermal limits (how hot the chips get).
  
  By the 1980s and 1990s, it became clear that scaling a single processor indefinitely wasn't going to work.
  Clock speeds stopped rising dramatically after the early 2000s because of these heat and power problems.
  So we stopped building wide, and started building tall.
  People started working with multiple processors and building distributed systems.
  Instead of making one super-powerful chip, we started connecting lots of smaller chips together.
  Thus avoiding the physical limits of single-chip designs, whilst still growing system size and performance.
  This is how concepts like cluster computing, parallel computing, and eventually cloud computing took off.
  Instead of making one super-powerful chip, we started connecting lots of smaller chips together.
  
  Famous early examples include Beowulf clusters (mid-1990s) for scientific computing, and Google's early distributed file systems and computing frameworks (like MapReduce, early 2000s) for handling web-scale data.
  
  The key takeaway from the classical distributed world beginings was:
  When scaling one machine hit a wall, we change the architecture.
  We network machines, distribute the workloads, and let many processors work together.
  
  The bells are pointing to a similar moment in quantum computing. 
  Except this time the rules of the game are even weirder.
  Quantum parallelism, entanglement, and superposition are not just fancy tricks to make things faster.
  They are the very fabric of quantum computing.
  And how they will behave in a set distributed open quantum systems (because afterall this is what quantum computers really are: open quantum systems) is a fascinating open question, that I hope to convince you is worth pursuing.
</p>

<h2> A Proven Classical Solution: Networks </h2>

<p> 
  In classical systems, when we network machines, we move bits around.
  You send them, store them, copy them.
  No drama.
  You just need to make sure your own logic doesn't eat its tale: avoid deadlocks, prevent sending the same bit twice, maintain cache consistency, etc.
  Some incredibly fancy protocols have been developed to ensure it all goes smoothly.
  Take a graduate distributed systems course will walk you through the basics.
  And although you will grind your teeth as you learn about Paxos, Raft, and other consenusalgorithms, you will eventually get thehang of it and know what problems to look out for (there is many, but they often come in the same shapes).
  
  On the other hand quantum systems, bring their own additional problems.
  Qubits can exist in superpositions. The game is no longer about dealing with combinations of zeros and ones, now we are working with physical states, that are arguably some of the most complex objects in the universe.
  They can be entangled across different nodes. We can now move bits around (if we work with photons for instance), AND we create additional correlations!
  The fancy teleportation lingo comes with a new onset of headache inducing problems.
  Also atop of gaining new power plays we lose some, you cannot just "read" a qubit and resend it without destroying it.
  Copying quantum information is forbidden (no-cloning theorem, Wootters and Zurek, 1982 Phys. Rev. D).
  We can move correlations around, but we can’t observe or copy them.

  Because of this, networked quantum computingintroduces fundamentally different problems from classical networked computing.
  This is not just an engineering problem.
  We have to deal with a whole new layer of physics we hadn't considered before.
</p>

<h2>New Quantum-Specific Questions</h2>

<p>
  What are the truly new challenges introduced by distributed quantum computing? 
  We can start with implementation-level concerns, which largely fall into two categories: <em>when</em> to connect machines, and <em>how</em> to do it.
</p>

<ul>
  <li>
    <strong>When should we connect quantum machines?</strong> 
    Not all quantum computations benefit from distribution. 
    The overhead of inter-device communication and coordination can easily outweigh the advantages.
    In fact, there is growing evidence that highly entangled regions of a quantum system are particularly fragile when partitioned, some may decohere beyond repair if distributed improperly.
    Identifying when distribution is beneficial, and when it is detrimental, remains an open question.
  </li>
  <li>
    <strong>Classical, Quantum or Hybrid networks?</strong> 
    Distributed quantum computing doesn't just involve quantum processors, it also requires networks.
    These networks can be classical (like fiber optics) or quantum (like photonic links).
    Each type of channel has its own set of advantages and disadvantages.
    Choosing between classical and quantum channels is a critical design decision.
    Classical communication is robust and well-understood, but only quantum links allow for entanglement distribution and teleportation.
    Each choice has trade-offs in terms of latency, fidelity, scalability, and error sensitivity.
    Understanding these trade-offs, and learning when to use which is still an active research area.
    For now most work has focused on trying to distribute outright, not on how this channel choice affects the computation.
  </li>
</ul>


<h2>  Parallelism: Lessons and Differences </h2>

<p>
  One of the other motivations behind classical distributed computing was the discovery of performance speedups through parallelism. 
  As early as the 1970s, Amdahl's Law formalized the limits of speedup in a system with a fixed sequential fraction, while Gustafson's Law in the 1980s reframed the problem for scaled workloads, showing that as we increase problem size, more of it can be parallelized. 
  These insights partially justified the shift to multi-core architectures and distributed systems, even when communication overhead was non-trivial.
</p>

<p>
  Quantum computing, however, doesn't present parallelism in the same way as classical systems. 
  Superposition and entanglement give quantum systems a kind of inherent parallelism. You'll often here the simplification that quantum parallelism give usexponentially many computational paths encoded in a single state. 
  But this is not true, quantum parallelism is not directly accessible: we cannot observe all paths simultaneously, nor can we arbitrarily manipulate individual branches.
  Also unlike classical computers, quantum computer can perform operations on distinct qubits at the same time. 
  Operational parallelism is inherent to the model.
  This makes it hard to argue for speedup via distribution in the same terms as classical parallelism.
  In fact, we currently expect the opposite to be true: distributing a quantum computation can introduce additional overhead, such as the need for error correction, qubit teleportation, and other forms of communication that can slow down the computation.
</p>

<p>
  Still, scientists are not easily discouraged.
  If you were to ask me, one of the many early in career researcher who are dedicating their PhD to this field, I believe (not without supporting evidence)
  that distributed quantum computing will offer new kinds of speedup: not by splitting work across nodes in the classical sense, but by allowing more complex entangled systems, enabling new types of measurements, or increasing algorithmic diversity through heterogeneity of nodes.
  When and how these speedups manifest is still poorly understood - ergo why we are eagerly working on it. 
</p>


<h2>Infrastructure Challenges</h2>

<p>
  Implementation challenges aside, there are also practical problems we can't ignore.
  Right now, we're working within a fractured quantum ecosystem.
  Superconducting qubits, trapped ions, photonics — each platform is racing to prove itself as the dominant one.
  They all come with strengths and tradeoffs, but there's no clear winner.
  Worse still, they don't speak the same language.
  Not everything runs on circuits! 
  Today's quantum computers run on different computational models. 
  From measurement-based quantum computing (MBQC) to topological models, these machines don't perform computations in the same way.
  They don't interoperate cleanly, and they certainly weren't designed to work together.
  Despite this hardware reality, most of the current distributed infrastructure work assumes circuit-based computation.
  But the moment you try cross between modalities, that assumption no longer holds.
</p>

<p>
  Ecosystem heterogeneity becomes a serious problem when we start distributing.
  Abstracting above or below the model is nice in theory — but today, distribution is anything but model-agnostic.
  And inter-model interoperability?
  Not even close.
  This is more than a hardware limitation — it's a conceptual mismatch between how we compute and how we communicate.
  Solving this will mean rethinking some of our core assumptions about distributed quantum computing.
  That's where some of my current work is heading.
  Keep an eye out for model-agnosticism on the horizon.
</p>


<h2>Current Strategies: Cutting and Knitting</h2>

<p>
  So what do I mean by <em>current distributed infrastructure assumes circuit-based computation</em>? 
  Let's break it down.
</p>

<p>
  The standard approach today is to take a quantum circuit (a sequence of gates acting on qubits) and split it into smaller parts that can run on different devices. 
  We usually model the circuit as a hypergraph, where qubit wires are nodes and gates are hyperedges connecting them.
  The goal is to solve a cutting problem: how can we partition this graph across devices while keeping communication costs low?
  This is an NP-hard problem, meaning even small circuits can produce difficult tradeoffs between cut size, depth, and overall fidelity.
</p>

<p>
  Once the circuit is cut, the pieces need to be "knitted" back together.
  This can happen in two main ways: we can cut qubit wires, which requires teleporting quantum states between devices, or we can cut gates, which involves creating long-range entanglement links to simulate the missing multi-qubit interaction.
  Both approaches are valid in theory.
  But they rely on real quantum networks with high-fidelity links, fast entanglement distribution, and precise coordination between remote devices.
  And right now, quantum networks for quantum compute are not yet at that level.
</p>

<p>
  The alternative we see available in industry today is to simulate these quantum links using classical communication.
  This works in simulation and in small experimental settings, but it's a bit like trying to run a Formula 1 car on a bicycle track.
  You can move forward, but the track isn't designed for that kind of speed or fidelity.
  You're constantly adapting the system to the limitations of classical infrastructure, instead of building infrastructure suited to quantum behavior.
  When we talk about circuit "knitting" today, we're often referring to this classical workaround, which is something of a misnomer.
  To emulate a quantum link over a classical channel, we run many shots on both sides of the cut and stitch together correlated measurement statistics.
  This allows us to reconstruct expectation values as if the circuit had been whole.
  But we aren't actually knitting quantum circuits together in the physical sense.
  We're approximating their output distributions using classical post-processing.
  Which racks up costs.
</p>

<h2>The Hardware conversation</h2>

<p>
  Many quantum platforms (especially superconducting qubits) were not built with quantum networking in mind.
  They're excellent for local, high-fidelity gates and fast control, but they cannot natively emit or absorb flying qubits like photons.
  To interface with a quantum network, superconducting qubits require an interconnect, a device that converts stationary qubits into photonic ones, typically via microwave-to-optical transduction (e.g., using resonators). 
  These interconnects are still in early stages.
  They introduce unavoidable losses, noise, and latency.
  The fidelity hit is significant, and the success rates for conversion remain low.
  Even the <em>"super"</em> in superconducting can't dodge these <em>super losses</em>.
  This creates a serious bottleneck.
  A device may perform well in isolation but become nearly unusable when asked to communicate with others.
  Until we have high-fidelity, low-loss interfaces / architectures that avoid the need for conversion entirely hardware will remain a core limitation for distributed quantum computing.
</p>


<h2>Recent Progress and Open Needs</h2>

<p>
  But despite this bleak state, the first real steps toward distributed quantum computing have been arriving!
  In early 2025, Xanadu introduced <strong>Aurora</strong> — the first modular, networked photonic quantum computer.
  Aurora connects 35 photonic chips via 13 kilometers of optical fiber across four server racks, forming a 12-qubit system designed for scalability and fault tolerance.
  This marks the first time a quantum computer has been built to be modular, networked, and truly scalable with quantum channels.
  And although it is still in the early stages, it represents a significant step toward a future where quantum computers can be distributed across multiple locations and connected via quantum networks.
</p>

<p>
  Around the same time, researchers at Oxford University demonstrated the first instance of distributed quantum computing by linking two separate quantum processors via a photonic network.
  They executed a distributed version of Grover's search algorithm, achieving an 86% fidelity for a teleported controlled-Z gate between modules and a 71% success rate for the algorithm.
  The experiment used trapped-ion QPUs and showed that non-trivial distributed algorithms are feasible even with current hardware.
</p>

<p>
  Another major development came from TU Delft, where researchers at QuTech and the Quantum Internet Alliance introduced <em>QNodeOS</em> — the first operating system for quantum networks.
  QNodeOS provides a hardware-agnostic abstraction layer, letting quantum applications issue high-level commands like entanglement generation or inter-node operations without being tied to specific devices.
  These requests are translated into hardware-specific instructions that coordinate timing, entanglement, and classical messaging between nodes.
  However, the system still assumes that each node runs gate-based quantum circuits.
  The computational model remains fixed, even if the hardware platform is abstracted.
  This is a major step forward, but it still leaves open the question of how to coordinate distributed computations across different models entirely.
</p>


<p>
  When all is said and done these are significant milestones, but much remains to be done:
</p>

<ul>
  <li>
    Conducting experiments on actual distributed computations, such as complex circuits, quantum walks, and MBQC patterns.
  </li>
  <li>
    Developing quantum network experiments that specifically target quantum computing needs, moving beyond network oriented applications like quantum key distribution (QKD).
  </li>
  <li>
    Demonstrating even short-distance server-to-server quantum communication, which is both urgent and impactful for validating distributed architectures.
  </li>
</ul>

<p>
  The field is progressing, but to realize the full potential of distributed quantum computing, we need to focus on these open challenges and continue building upon these foundational experiments.
</p>


<h2>Closing: Why I Work on DQC</h2>

<p>
  Distributed Quantum Computing isn't just a niche topic. 
  It's essential if we want to scale quantum computing beyond the limits of single devices.
  The physical constraints are real, and they won't be solved by squeezing more qubits onto a chip.
</p>

<p>
  DQC forces us to ask hard questions: 
  How do we coordinate entanglement across distance?
  How do we distribute algorithms across heterogeneous platforms?
  How do we rethink hardware and software to support modular, network-aware architectures?
</p>

<p>
  These aren't engineering challenges, they're fundamental questions about what it means to compute in a quantum world.
  And they're questions we need to answer if we want quantum computing to move from lab demos to real-world impact.
</p>

<p>
  That's why I work on DQC.
  It's one of the few paths forward that can take quantum computing to its next phase.
</p>

