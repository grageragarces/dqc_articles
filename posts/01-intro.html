<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Why am I working on Distributed Quantum Computing (DQC)?</title>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <article>
    <h1>Why am I working on Distributed Quantum Computing (DQC)?</h1>
    <p><em>May 2025</em></p>

    <p>
      Distributed Quantum Computing (DQC) offers a promising path to scale quantum capabilities beyond the limitations of single devices.
      In this post, I share the motivation behind my work in this area:
      why distributing quantum computations matters, what challenges it poses,
      and how it connects to real limitations in current hardware.
      This sets the stage for a broader discussion on how we can externalize,
      coordinate, and benchmark quantum computation across multiple devices.
    </p>

    <p><a href="../index.html">← Back to all posts</a></p>
  </article>
</body>

<p>
  Building ever-larger quantum computers by simply adding more qubits into a single device isn't a sustainable strategy. 
  Let's look at why: 
</p>
<p>
  No matter how small we make qubits, we're up against fundamental physical constraints.
  For instance the "area law" in quantum physics, 
  suggests that the entanglement entropy (a measure of quantum correlations) of a subsystem scales with the boundary area of the subsystem. 
  This concept was first introduced in the context of black hole thermodynamics by physicists Jacob Bekenstein and Stephen Hawking in the 1970s, who found that the entropy of a black hole is proportional to the area of its event horizon. 
  Implying that as we try to pack more qubits into a single device, the complexity of managing entanglement will grow rapidly (with the surface area of subsystems), making it increasingly difficult to maintain coherent quantum states.
  
  Decoherence is already a huge problem in current quantum computers, where achieving fault tolerance requires significant overhead - the entire field of QEC (Quantum Error Correction), was built to fight these errors. 
  QEC tries to manipulate quantum objects, which are inherently fragile and sensitive to their environment.
  Implementing error correction protocols necessitates additional physical qubits to encode a single logical qubit, increasing the total qubit counts. 
  This exacerbates the spatial constraints and introduces additional complexity in qubit control and coherence maintenance.
  We expect to reach a point where adding more qubits to a single device will yield diminishing returns, if not outright failure, due to the inability to manage the entanglement and coherence of these incredibly complex systems.
  Building wide forever is not viable, we must build tall, distributing computations across multiple devices, allowing us to scale up without hitting these physical limits.
</p>

<p>
  These challenges are reminiscent of the obstacles faced in classical computing during the late 20th century. 
  In the late 20th century, classical computers hit expected physical limits - similar to the ones we are predicting in the quantum world.
  Moore's law kept pushing the number of transistors up, but at some point, packing more into a chip caused too much heat and reliability issues.
  Processors started to run into both physical limits (how small you can make features) and thermal limits (how hot the chips get).
  
  By the 1980s and 1990s, it became clear that scaling a single processor indefinitely wasn't going to work.
  Clock speeds stopped rising dramatically after the early 2000s because of these heat and power problems.
  So we stopped building wide, and started building tall.
  People started working with multiple processors and building distributed systems.
  Instead of making one super-powerful chip, we started connecting lots of smaller chips together.
  Thus avoiding the physical limits of single-chip designs, whilst still growing system size and performance.
  This is how concepts like cluster computing, parallel computing, and eventually cloud computing took off.
  Instead of making one super-powerful chip, we started connecting lots of smaller chips together.
  
  Famous early examples include Beowulf clusters (mid-1990s) for scientific computing, and Google's early distributed file systems and computing frameworks (like MapReduce, early 2000s) for handling web-scale data.
  
  The key takeaway from the classical distributed world beginings was:
  When scaling one machine hit a wall, we change the architecture.
  We network machines, distribute the workloads, and let many processors work together.
  
  The bells are pointing to a similar moment in quantum computing. 
  Except this time the rules of the game are even weirder.
  Quantum parallelism, entanglement, and superposition are not just fancy tricks to make things faster.
  They are the very fabric of quantum computing.
  And how they will behave in a set distributed open quantum systems (because afterall this is what quantum computers really are: open quantum systems) is a fascinating open question, that I hope to convince you is worth pursuing.
</p>

<h2> A Proven Classical Solution: Networks </h2>

<p> 
  In classical systems, when we network machines, we move bits around.
  You send them, store them, copy them.
  No drama.
  You just need to make sure your own logic doesn't eat its tale: avoid deadlocks, make sure you don't send the same bit twice, don't mess up your own cache, etc.
  Some incredibly fancy protocols have been developed to ensure it all goes smoothly.
  Take a graduate distributed systems course will walk you through the basics.
  And although you will grind your teeth as you learn about Paxos, Raft, and other consensus algorithms, you will eventually get the hack of it and know what problems to look out for (there is many, but they often come in the same shapes).
  
  On the other hand quantum systems, bring their own additional problems.
  Qubits can exist in superpositions. The game is no longer about dealing with combinations of zeros and ones, now we are working with physical states, that are arguably some of the most complex objects in the universe.
  They can be entangled across different nodes. We can now move bits around (if we work with photons for instance), AND we create additional correlations!
  The fancy teleportation lingo comes with a new onset of headache inducing problems.
  Also atop of gaining new power plays we lose some, you cannot just "read" a qubit and resend it without destroying it.
  Copying quantum information is forbidden(no-cloning theorem, Wootters and Zurek, 1982 Phys. Rev. D).
  We now get to move correlatons around, but we can't look at them.

  Because of this, networked quantum computing brings along fundamentally different problems from classical networked computing.
  This is not just an engineering problem.
  We have to deal with a whole new layer of physics we hadn't considered before.
</p>

<h2>New Quantum-Specific Questions</h2>

<p>
  What are the truly new challenges introduced by distributed quantum computing? 
  We can start with implementation-level concerns, which largely fall into two categories: <em>when</em> to connect machines, and <em>how</em> to do it.
</p>

<ul>
  <li>
    <strong>When should we connect quantum machines?</strong> 
    Not all quantum computations benefit from distribution. 
    The overhead of inter-device communication and coordination can easily outweigh the advantages.
    In fact, there is growing evidence that highly entangled regions of a quantum system are particularly fragile when partitioned, some may decohere beyond repair if distributed improperly.
    Identifying when distribution is beneficial, and when it is detrimental, remains an open question.
  </li>
  <li>
    <strong>Classical, Quantum or Hybrid networks?</strong> 
    Distributed quantum computing doesn't just involve quantum processors, it also requires networks.
    These networks can be classical (like fiber optics) or quantum (like photonic links).
    Each type of channel has its own set of advantages and disadvantages.
    Choosing between classical and quantum channels is a critical design decision.
    Classical communication is robust and well-understood, but only quantum links allow for entanglement distribution and teleportation.
    Each choice has trade-offs in terms of latency, fidelity, scalability, and error sensitivity.
    Understanding these trade-offs, and learning when to use which is still an active research area.
    For now most work has focused on trying to distribute outright, now on how this channel choice affects the computation.
  </li>
</ul>


<h2>  Parallelism: Lessons and Differences </h2>

<p>
  One of the other motivations behind classical distributed computing was the discovery of performance speedups through parallelism. 
  As early as the 1970s, Amdahl's Law formalized the limits of speedup in a system with a fixed sequential fraction, while Gustafson’s Law in the 1980s reframed the problem for scaled workloads, showing that as we increase problem size, more of it can be parallelized. 
  These insights partially justified the shift to multi-core architectures and distributed systems, even when communication overhead was non-trivial.
</p>

<p>
  Quantum computing, however, doesn’t present parallelism in the same way as classical systems. 
  Superposition and entanglement give quantum systems a kind of inherent parallelism. You'll often here the simplification that quantum parallelism give sus exponentially many computational paths encoded in a single state. 
  But this is not true, quantum parallelism is not directly accessible: we cannot observe all paths simultaneously, nor can we arbitrarily manipulate individual branches.
  Also unlike classical computers, quantum computer can perform operations on distinct qubits at the same time. 
  Operational parallelism is inherent to the model.
  This makes it hard to argue for speedup via distribution in the same terms as classical parallelism.
  In fact, we currently expect the opposite to be true: distributing a quantum computation can introduce additional overhead, such as the need for error correction, qubit teleportation, and other forms of communication that can slow down the computation.
</p>

<p>
  Still, it is not typical of scientists to despair this easly.
  If you were to ask me, one of the many early in career researcher who are dedicating their PhD to this field, I believe (not without supporting evidence)
  that distributed quantum computing will offer new kinds of speedup: not by splitting work across nodes in the classical sense, but by allowing more complex entangled systems, enabling new types of measurements, or increasing algorithmic diversity through heterogeneity of nodes.
  When and how these speedups manifest is still poorly understood - ergo why we are eagerly working on it. 
</p>


<h2>  Infrastructure Challenges </h2>

    Our entire distributed quantum computing infrastructure assumes circuit-based models.

    Circuit models are not naturally compatible with photons — the main transportable qubit.

    This is a major limitation and an area where improvements are needed (hint at your upcoming work).

<h2>  Current Strategies: Cutting and Knitting </h2>

    To distribute circuits today:

        We model circuits as hypergraphs and solve a cutting problem (NP-hard).

        We can either:

            Cut qubit wires (teleportation between QPUs),

            Cut gates (long-distance entanglement-assisted gates).

    Both strategies depend on real quantum networks — which are underdeveloped.

<h2>   Near-Term Approximations </h2>

    Many experiments today "simulate" distributed quantum computing:

        Using probabilistic quasi-channel decompositions.

        Good for small systems (hundreds of qubits), but scales poorly.

<h2>   Hardware Limitations </h2>

    Many platforms (especially superconducting qubits) cannot directly interact with quantum networks.

    Superconducting setups need interconnects to convert to photons, introducing unavoidable losses.

    (Optional pun: "Even the 'super' in superconducting can't dodge these super losses.")

<h2>   Recent Progress and Open Needs </h2>

    First real steps: Xanadu's Aurora, the first distributed quantum computer.

    Much remains:

        Experiments on actual distributed computations (circuits, quantum walks, MBQC patterns).

        Quantum network experiments specifically targeting quantum compute needs, not just QKD.

        Even short-distance server-to-server demonstrations are urgent and impactful.

<h2>   Closing: Why I Work on DQC </h2>

    DQC is essential for real quantum computing at scale.

    It demands solving deep new questions in networking, algorithms, and hardware design.

    It is one of the few ways forward to push quantum computing beyond current limitations.
</html>
